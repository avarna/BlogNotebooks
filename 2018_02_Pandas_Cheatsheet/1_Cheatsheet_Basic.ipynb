{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data from different file formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df1 = pd.read_csv('Kaggle_Titanic_csv.csv') # Read from csv file\n",
    "df2 = pd.read_excel('Kaggle_Titanic_excel.xlsx') # Read from excel file\n",
    "df3 = pd.read_json('Kaggle_Titanic_json.json') # Read from json file\n",
    "\n",
    "# Read tables from a URL\n",
    "dfs = pd.read_html('https://en.wikipedia.org/wiki/List_of_state_and_union_territory_capitals_in_India', header=0)\n",
    "# \"header=0\" uses the first row of the table as the header\n",
    "\n",
    "# Above command reads all the tables and providea a \"list\" of tables. Select the one you want as shown below\n",
    "df4 = dfs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing the dataframe (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 11 columns):\n",
      "PassengerId    200 non-null int64\n",
      "Pclass         200 non-null int64\n",
      "Name           200 non-null object\n",
      "Sex            200 non-null object\n",
      "Age            160 non-null float64\n",
      "SibSp          200 non-null int64\n",
      "Parch          200 non-null int64\n",
      "Ticket         200 non-null object\n",
      "Fare           199 non-null float64\n",
      "Cabin          45 non-null object\n",
      "Embarked       200 non-null object\n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 17.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df1.shape # Number of rows and columns\n",
    "df1.shape[0] # Number of rows\n",
    "len(df1) # same as above\n",
    "df1.shape[1] # Number of columns\n",
    "\n",
    "df1.size # Number of entries/cells in the df\n",
    "\n",
    "df1.columns # Column names\n",
    "\n",
    "df1.head(3) # Display first 3 rows of the df\n",
    "df1.tail(3) # Display last 3 rows of the df\n",
    "\n",
    "df1.info() # General info about the df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " PassengerID data type before =  float64  and after =  float64\n"
     ]
    }
   ],
   "source": [
    "# df1.columns = ['Col1','Col2','Col3'] # Rename all the columns of a dataframe\n",
    "df1.rename(columns = {'Pclass':'Class', 'Fare':'Price'}) # Rename selected columns\n",
    "\n",
    "df1['Pclass'].unique() # unique values present in a column\n",
    "\n",
    "df1.isnull() # Returns a dataframe containing boolean values (True if NaN)\n",
    "df1.notnull() # Opposite of df.isnull()\n",
    "\n",
    "df1.isnull().any() # Tells whether a column has missing values. More useful than \"df.isnull()\"\n",
    "df1.isnull().sum() # Gives the number of missing entries in each column\n",
    "\n",
    "df1.dropna() # Drop ROWS having missing values\n",
    "df1.dropna(axis=1) # Drop COLUMNS having missing values\n",
    "\n",
    "df1['Age'].fillna(10) # Fill the blanks in 'Age' columns with 10\n",
    "df1['Age'].fillna(df1['Age'].median()) # Fill the blanks in 'Age' columns with its median value\n",
    "\n",
    "df1.drop_duplicates() # Drop duplicate rows\n",
    "\n",
    "df1['Pclass'].replace(1,'A') #  Replace values in a column\n",
    "df1['Pclass'].replace([2,3],['B','C'])\n",
    "\n",
    "# Convert data type\n",
    "print(\"\\n PassengerID data type before = \", df1['Age'].dtype, \" and after = \", df1['Age'].astype(float).dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics\n",
    "\n",
    "**df.describe()** : Summary of statistics\n",
    "\n",
    "**df.count()** : Count of numerical values in each column\n",
    "\n",
    "**df.max()** : Maximum value in each column\n",
    "\n",
    "**df.min()** : Miniimum value in each column\n",
    "\n",
    "**df.mean()** : Mean value in each column\n",
    "\n",
    "**df.median()** : Median value in each column\n",
    "\n",
    "**df.var()** : Variance in each column\n",
    "\n",
    "**df.std()** : Standard deviation in each column\n",
    "\n",
    "**df.corr()** : Correlation coefficient between different columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection and subset dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>1071</td>\n",
       "      <td>1</td>\n",
       "      <td>Compton, Mrs. Alexander Taylor (Mary Eliza Ing...</td>\n",
       "      <td>female</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>PC 17756</td>\n",
       "      <td>83.1583</td>\n",
       "      <td>E45</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>1047</td>\n",
       "      <td>3</td>\n",
       "      <td>Duquemin, Mr. Joseph</td>\n",
       "      <td>male</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S.O./P.P. 752</td>\n",
       "      <td>7.5500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>915</td>\n",
       "      <td>1</td>\n",
       "      <td>Williams, Mr. Richard Norris II</td>\n",
       "      <td>male</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PC 17597</td>\n",
       "      <td>61.3792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1051</td>\n",
       "      <td>3</td>\n",
       "      <td>Peacock, Mrs. Benjamin (Edith Nile)</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>SOTON/O.Q. 3101315</td>\n",
       "      <td>13.7750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Pclass                                               Name  \\\n",
       "179         1071       1  Compton, Mrs. Alexander Taylor (Mary Eliza Ing...   \n",
       "155         1047       3                               Duquemin, Mr. Joseph   \n",
       "23           915       1                    Williams, Mr. Richard Norris II   \n",
       "159         1051       3                Peacock, Mrs. Benjamin (Edith Nile)   \n",
       "\n",
       "        Sex   Age  SibSp  Parch              Ticket     Fare Cabin Embarked  \n",
       "179  female  64.0      0      2            PC 17756  83.1583   E45        C  \n",
       "155    male  24.0      0      0       S.O./P.P. 752   7.5500   NaN        S  \n",
       "23     male  21.0      0      1            PC 17597  61.3792   NaN        C  \n",
       "159  female  26.0      0      2  SOTON/O.Q. 3101315  13.7750   NaN        S  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SELECTION by POSITION (only integer values)\n",
    "df1.iloc[0,:] # Entire 1st row\n",
    "df1.iloc[0] # same as above\n",
    "df1.iloc[0:3,:] # First two rows\n",
    "\n",
    "df1.iloc[:,0] # Entire 1st column\n",
    "df1.iloc[0,0] # Element of 1st row and 1st column\n",
    "df1.iloc[:,-2:] # Last two columns\n",
    "\n",
    "\n",
    "# SELECTION by LABELS/INDEX\n",
    "df1.loc[1,:] # Entire row with index '1' (2nd row)\n",
    "df1.loc[1] # same as above\n",
    "df1.loc[:,'Age'] # 'Age' column\n",
    "df1.loc[0,'Age'] # First element of 'Age' column\n",
    "df1.loc[:,'Age':'Fare'] # columns from 'Age' till 'Fare'\n",
    "\n",
    "\n",
    "# SUBSETS\n",
    "df1['Age'] # subset dataframe containing only 'Age' column\n",
    "df1.Age # same as above\n",
    "df1_a = df1[['Age','Fare']] # subset dataframe containing 'Age' & 'Fare' columns\n",
    "\n",
    "# Select columns whose name contains certain letters\n",
    "df1.filter(regex='Id$') # Column names ENDING with \"Id\"\n",
    "df1.filter(regex='^P') # Column names BEGINNING with \"P\"\n",
    "\n",
    "df1[(df1.Age>10) & (df1.Age<50)] # Select rows whose 'Age' > 10 AND <50\n",
    "df1.loc[(df1.Age>10) & (df1.Age<50)] # Same as above\n",
    "df1.loc[df1.Age>10, ['Pclass', 'Fare']] # Select rows whose 'Age' > 10 AND only Pclass & Fare columns\n",
    "\n",
    "df1.sample(frac=0.5, random_state = 7) # Randomly select 50% of the rows. \n",
    "df1.sample(n=4, random_state = 13) # Randomly select 4 rows\n",
    "# Set the random_state to some value to produce repeatable results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifying df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fare</th>\n",
       "      <th>Pclass</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>female</th>\n",
       "      <td>21.00000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>13.20835</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Fare  Pclass\n",
       "Sex                     \n",
       "female  21.00000       3\n",
       "male    13.20835       3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.transpose() # Transpose the dataframe\n",
    "\n",
    "df1['Fare By Age'] = df1['Fare']/df1.Age # ADD new columns\n",
    "\n",
    "df1.drop('Fare By Age', axis=1, inplace=True) # \"inplace=True\" deletes column in df1.  \n",
    "df1_b = df1.drop(['Name','Ticket','Cabin'], axis=1) # Drop the given columns and display (but does not delete it from df1)\n",
    "\n",
    "df1_b.drop(df1_b.index[-2:], inplace=True) # drop last two rows (default axis=0 => drop rows)\n",
    "\n",
    "df1.sort_values('Age') # sort rows based on Age (low to high)\n",
    "df1.sort_values('Age', ascending=False) # sort rows based on Age (high to low)\n",
    "\n",
    "df1.reset_index(drop=True) # gives new indices in ascending order from 0\n",
    "df1.reset_index() # By default \"ddf1.drop(['Name','Ticket','Cabin'], axis=1)rop=False\" => the existing index is used as another column\n",
    "\n",
    "\n",
    "# Just \"df1.groupby('Sex')\" produces an object, not a dataframe.\n",
    "# Uses any statistical function mentioned above to get a dataframe\n",
    "\n",
    "df1.groupby('Sex').mean() # use 'Sex' as index abd calculate mean of other columns\n",
    "df1.groupby('Sex').agg(np.mean) # same as above. Functions not available in Pandas can be applied using \"agg()\" \n",
    "\n",
    "df1.groupby(['Sex','Pclass']).median() # use 'Sex' and 'Pclass' as indices and calculate median of other columns\n",
    "\n",
    "# Use 'Sex' as an index and calculate median of only 'Pclass' and 'Fare' columns\n",
    "df1.groupby('Sex')['Pclass','Fare'].median() \n",
    "df1.pivot_table(index='Sex', values=['Pclass','Fare'], aggfunc=np.median) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# append, concat, join, merge\n",
    "**append()** is a special case of **concat()** . If you don't want to remember too many functions, forget **append()**.\n",
    "\n",
    "**merge()** and **join()** are for SQL-style JOINing operations of TWO dataframes. (See the comparison with SQL here: https://pandas.pydata.org/pandas-docs/stable/comparison_with_sql.html#compare-with-sql-join ) \n",
    "\n",
    "**join()** is much more convenient and is applicable for dataframes having different indices. However, one can just forget **join()** and get the job done with **merge()** .\n",
    "\n",
    "Information about the differences between these functions are available at:\n",
    "<br>\n",
    "1) https://www.reddit.com/r/learnpython/comments/6986nd/difference_between_concatenate_append_merge_in/\n",
    "<br>\n",
    "2) http://py-tut.blogspot.in/2016/11/pandas-concat-and-append.html\n",
    "\n",
    "Graphical explanations about:\n",
    "<br>\n",
    "1) merge: https://github.com/pandas-dev/pandas/blob/master/doc/cheatsheet/Pandas_Cheat_Sheet.pdf\n",
    "<br>\n",
    "2) All functions: https://pandas.pydata.org/pandas-docs/stable/merging.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes of dataframes before concat = (200, 2) (198, 8)\n",
      "shapes of dataframes after (default i.e 'outer'/union) concat = (200, 10)\n",
      "shapes of dataframes after 'inner' (intersection) concat = (198, 10)\n"
     ]
    }
   ],
   "source": [
    "df_big = pd.concat([df1,df2,df3], ignore_index=True) # Join three dfs along rows (similar to append() function)\n",
    "\n",
    "# \"axis=1\" => Join dataframes along columns\n",
    "df1_union = pd.concat([df1_a,df1_b], axis=1) # Default join => NaN in place of missing rows/columns \n",
    "df1_intersect = pd.concat([df1_a,df1_b], axis=1, join='inner')\n",
    "\n",
    "print(\"shapes of dataframes before concat =\", df1_a.shape, df1_b.shape)\n",
    "print(\"shapes of dataframes after (default i.e 'outer'/union) concat =\", df1_union.shape)\n",
    "print(\"shapes of dataframes after 'inner' (intersection) concat =\", df1_intersect.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing data to different file formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to a csv file\n",
    "df2.to_csv('Output_csv.csv', index=False) \n",
    "# \"index=False\" avoids the extra \"unnamed:0\" column\n",
    "\n",
    "# Writing to an excel file\n",
    "xl_filename = pd.ExcelWriter('Output_excel.xlsx')\n",
    "df2.to_excel(xl_filename, 'Titanic data', index=False)\n",
    "xl_filename.save()\n",
    "\n",
    "# Writing to a json file\n",
    "df2 = df2.to_json(orient='records', force_ascii=False, date_format='iso')\n",
    "with open('Output_json.json', 'w') as f:\n",
    "    f.write(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1) https://www.dataquest.io/blog/pandas-cheat-sheet/\n",
    "    \n",
    "2) https://github.com/pandas-dev/pandas/blob/master/doc/cheatsheet/Pandas_Cheat_Sheet.pdf\n",
    "\n",
    "NOTE: The data used here is the Titanic dataset from Kaggle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
