{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python script to determine the sentiment of an article based on the\n",
    "number of positive and negative words.\n",
    "\n",
    "The dictionaries of positive and negative words are from:\n",
    "<br>\n",
    "1) https://www3.nd.edu/~mcdonald/Word_Lists.html : words specific to finance\n",
    "<br>\n",
    "2) https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#datasets : general words\n",
    "\n",
    "The idea is derived from:\n",
    "http://francescopochetti.com/financial-blogs-sentiment-analysis-part-crawling-web/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of word count based sentiment =  0.577607113985\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from text_preprocess import remove_symbol, remove_stopword\n",
    "\n",
    "np.random.seed(7) # Fix random seed for reproducibility\n",
    "\n",
    "\"\"\"\n",
    "_____________________\n",
    "Step 1: Load articles\n",
    "_____________________\n",
    "\"\"\"\n",
    "# The data is mined from coindesk\n",
    "data = pd.read_json('data/manualVerified_senti_2600.json')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "_________________________\n",
    "Step 2: Load dictionaries\n",
    "_________________________\n",
    "\"\"\"\n",
    "\n",
    "dict_1 = pd.read_excel('data/WordDatabase/1/LoughranMcDonald_MasterDictionary_2014.xlsx')\n",
    "\n",
    "poswd_1 = [x['Word'].lower() for i,x in dict_1.iterrows() if x['Positive']]\n",
    "\n",
    "# str() is necessary to use .lower() on the word 'False'\n",
    "# Otherwise 'False' will be treated as boolean and gives error with .lower()\n",
    "negwd_1 = [str(x['Word']).lower() for i,x in dict_1.iterrows() if x['Negative']]\n",
    "\n",
    "poswd_1 = pd.DataFrame(columns=['Positive'], data=poswd_1)\n",
    "negwd_1 = pd.DataFrame(columns=['Negative'], data=negwd_1)\n",
    "\n",
    "\n",
    "# Load positive and negative words from 2nd dictionary source\n",
    "poswd_2 = pd.read_csv('data/WordDatabase/2/positive-words.txt', \n",
    "                      sep=\" \", comment=';', names=['Positive']) \n",
    "\n",
    "# encoding is required here since some words have '-'\n",
    "negwd_2 = pd.read_csv('data/WordDatabase/2/negative-words.txt', \n",
    "                      sep=\" \", comment=';', names=['Negative'], encoding='latin-1') \n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "_________________________\n",
    "Step 3: Clean the data\n",
    "_________________________\n",
    "\"\"\"\n",
    "\n",
    "# Remove \\xa0\n",
    "data['contents'] = data['contents'].apply(lambda x: x.replace(u'\\xa0', u' '))\n",
    "\n",
    "# Remove text after \"The leader in blockchain news\"\n",
    "data['contents'] = data['contents'].apply(lambda x: x.split('The leader in blockchain news')[0])\n",
    "data['contents'] = data['contents'].apply(lambda x: x.split('Disclosure:')[0])\n",
    "data['contents'] = data['contents'].apply(lambda x: x.split('Disclaimer:')[0])\n",
    "\n",
    "# Remove punctuations and stopwords (a, the, is ...)\n",
    "data['contents clean'] = data['contents'].apply(remove_symbol).apply(remove_stopword)\n",
    "\n",
    "# Tokeniz the sentence into a list of words\n",
    "data['contents tokens'] = data['contents clean'].apply(lambda x: x.split())\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "_______________________________________________________________\n",
    "Step 4: Determine sentiment based on two different dictionaries\n",
    "_______________________________________________________________\n",
    "\"\"\"\n",
    "\n",
    "# Define functions to measure the sentiment\n",
    "def count_words(txt, diction):\n",
    "    # Function to count words in \"txt\" that are in \"diction\"\n",
    "    # CAUTION: 'diction' should be a list, not a pandas dataframe\n",
    "    # \"if word in diction\" works ONLY for a list\n",
    "    wrds = [word for word in txt if word in diction]\n",
    "    return len(wrds)\n",
    "\n",
    "def measure_senti(txt, neg_dict, pos_dict):\n",
    "    return(count_words(txt,pos_dict) - count_words(txt,neg_dict))\n",
    "\n",
    "\n",
    "# Convert negwd_2['Negative'] to a list. Doesn't work otherwise\n",
    "data['Senti 1'] = data['contents tokens'].apply(\n",
    "    lambda x: measure_senti(x,negwd_1['Negative'].tolist(), poswd_1['Positive'].tolist()))\n",
    "\n",
    "data['Senti 2'] = data['contents tokens'].apply(\n",
    "    lambda x: measure_senti(x,negwd_2['Negative'].tolist(), poswd_2['Positive'].tolist()))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "_____________________________________\n",
    "Step 5: Determine the final sentiment\n",
    "_____________________________________\n",
    "\"\"\"\n",
    "\n",
    "def measure_finalSenti(row):\n",
    "    \"\"\"\n",
    "    Positive(+1): when sentiment from both dictionaries are positive\n",
    "    Negative(-1): when sentiment from both dictionaries are negative\n",
    "    Neutral(0): Otherwise\n",
    "    \"\"\"\n",
    "    if (row['Senti 1']*row['Senti 2'] >0):\n",
    "        if row['Senti 1']>0:\n",
    "            senti = 1\n",
    "        else:\n",
    "            senti = -1\n",
    "    else:\n",
    "        senti = 0\n",
    "    \n",
    "    return senti\n",
    "\n",
    "data['Sentiment'] = data.apply(measure_finalSenti, axis=1) \n",
    "#axis=1 option sends the entire row to 'measure_finalsenti' function\n",
    "\n",
    "data_final = data[['id','title','contents','Sentiment']]\n",
    "\n",
    "data_final = data_final.to_json(orient='records',force_ascii=False)\n",
    "\n",
    "with open('data/data_senti_dum.json', 'w') as f:\n",
    "    f.write(data_final)\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "_____________________________________________________________\n",
    "Step 6: Measure the accuracy of sentiment based on word count\n",
    "_____________________________________________________________\n",
    "\"\"\"   \n",
    "\n",
    "data_valid = pd.read_json('data/manualVerified_senti_2600.json')\n",
    "data_wordcount = pd.read_json('data/data_senti_dum.json')\n",
    "\n",
    "accuracy = (data_wordcount['Sentiment']==data_valid['Sentiment']).sum()/len(data_valid)\n",
    "print(\"Accuracy of word count based sentiment = \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "The first step in training a neural network model is to label the available data. It is time-consuming for a human to read thousands of news articles and label each of them as positive, neutral or negative article. One way to determine the sentiment of an article is to count the number of positive (increase, welcome, gain etc.) and negative (ban, scam, tough etc.) in a news article. It is easy to write a Python script to do this work and this technique does not need any \"labelled\" training data. However, it will not be accurate since it cannot capture negations (\"not good\" will be counted as one positive and one negative word, and the net sentiment would be \"neutral\" which is not true. Bi-grams can be used to avoid this issue but it is computationally expensive and is still prone to other errors). \n",
    "\n",
    "We can use the above script to determine the sentiment of articles based on word count. Later, the labels assigned by this code can be verified manually to determine the accuracy of this technique. As shown above, the word-count technique has 58% accuracy which is quite good! The accuracy of a baseline technique (random guess) will be 33% since there are 3 categories.\n",
    "\n",
    "Why use this word-count technique when we have to manually verify them anyways? We can just skim the article quickly while correcting the sentiment whereas we have to read the article thoroughly when we have no prior idea about the sentiment of the article. This saves a lot of time!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
